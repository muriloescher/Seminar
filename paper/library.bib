@misc{risse2024scorewrongexambenchmarking,
      title={Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection}, 
      author={Niklas Risse and Marcel Böhme},
      year={2024},
      eprint={2408.12986},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2408.12986}, 
}

@inproceedings{10.1145/3533767.3534380,
author = {Lipp, Stephan and Banescu, Sebastian and Pretschner, Alexander},
title = {An empirical study on the effectiveness of static C code analyzers for vulnerability detection},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534380},
doi = {10.1145/3533767.3534380},
abstract = {Static code analysis is often used to scan source code for security vulnerabilities. Given the wide range of existing solutions implementing different analysis techniques, it is very challenging to perform an objective comparison between static analysis tools to determine which ones are most effective at detecting vulnerabilities. Existing studies are thereby limited in that (1) they use synthetic datasets, whose vulnerabilities do not reflect the complexity of security bugs that can be found in practice and/or (2) they do not provide differentiated analyses w.r.t. the types of vulnerabilities output by the static analyzers. Hence, their conclusions about an analyzer's capability to detect vulnerabilities may not generalize to real-world programs. In this paper, we propose a methodology for automatically evaluating the effectiveness of static code analyzers based on CVE reports. We evaluate five free and open-source and one commercial static C code analyzer(s) against 27 software projects containing a total of 1.15 million lines of code and 192 vulnerabilities (ground truth). While static C analyzers have been shown to perform well in benchmarks with synthetic bugs, our results indicate that state-of-the-art tools miss in-between 47\% and 80\% of the vulnerabilities in a benchmark set of real-world programs. Moreover, our study finds that this false negative rate can be reduced to 30\% to 69\% when combining the results of static analyzers, at the cost of 15 percentage points more functions flagged. Many vulnerabilities hence remain undetected, especially those beyond the classical memory-related security bugs.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {544–555},
numpages = {12},
keywords = {empirical study, static code analysis, vulnerability detection},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@INPROCEEDINGS{BugOSS,
     author={Kim, Jeewoong and Hong, Shin},
     booktitle={IEEE International Conference on Software Testing, Verification, and Validation (ICST)}, 
     title={{Poster}: {BugOSS}: A Regression Bug Benchmark for Evaluating Fuzzing Techniques}, 
     year={2023}
}

@misc{codeql,
  title = {CodeQL},
  author = {GitHub},
  howpublished = {\url{https://codeql.github.com/}},
  note = {Accessed: 2025-01-25}
}

@misc{infer,
  title = {Infer: A Tool to Detect Bugs in Java and C/C++/Objective-c Code},
  author = {Meta},
  howpublished = {\url{https://fbinfer.com/}},
  note = {Accessed: 2025-01-25}
}

@misc{li2024llmassistedstaticanalysisdetecting,
      title={LLM-Assisted Static Analysis for Detecting Security Vulnerabilities}, 
      author={Ziyang Li and Saikat Dutta and Mayur Naik},
      year={2024},
      eprint={2405.17238},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2405.17238}, 
}

@misc{keltek2024boostingcybersecurityvulnerabilityscanning,
      title={Boosting Cybersecurity Vulnerability Scanning based on LLM-supported Static Application Security Testing}, 
      author={Mete Keltek and Rong Hu and Mohammadreza Fani Sani and Ziyue Li},
      year={2024},
      eprint={2409.15735},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2409.15735}, 
}

@misc{du2024vulragenhancingllmbasedvulnerability,
      title={Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG}, 
      author={Xueying Du and Geng Zheng and Kaixin Wang and Jiayi Feng and Wentai Deng and Mingwei Liu and Bihuan Chen and Xin Peng and Tao Ma and Yiling Lou},
      year={2024},
      eprint={2406.11147},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.11147}, 
}

@misc{zhou2024comparisonstaticapplicationsecurity,
      title={Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection}, 
      author={Xin Zhou and Duc-Manh Tran and Thanh Le-Cong and Ting Zhang and Ivana Clairine Irsan and Joshua Sumarlin and Bach Le and David Lo},
      year={2024},
      eprint={2407.16235},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.16235}, 
}

@INPROCEEDINGS{8804441,
  author={Yang, Jinqiu and Tan, Lin and Peyton, John and A Duer, Kristofer},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={Towards Better Utilizing Static Application Security Testing}, 
  year={2019},
  volume={},
  number={},
  pages={51-60},
  keywords={Cross-site scripting;Java;Testing;SQL injection;Software;Visualization;static application security testing;static bug detection;utilization of software engineering tools;software reliability},
  doi={10.1109/ICSE-SEIP.2019.00014}
}

@INPROCEEDINGS {ding2024vulnerabilitydetectioncodelanguage,
author = { Ding, Yangruibo and Fu, Yanjun and Ibrahim, Omniyyah and Sitawarin, Chawin and Chen, Xinyun and Alomair, Basel and Wagner, David and Ray, Baishakhi and Chen, Yizheng },
booktitle = { 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE) },
title = {{ Vulnerability Detection with Code Language Models: How Far Are We? }},
year = {2025},
volume = {},
ISSN = {1558-1225},
pages = {469-481},
abstract = { In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection. To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions. Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain. },
keywords = {},
doi = {10.1109/ICSE55347.2025.00038},
url = {https://doi.ieeecomputersociety.org/10.1109/ICSE55347.2025.00038},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}

@misc{sun2025llm4vulnunifiedevaluationframework,
      title={LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning}, 
      author={Yuqiang Sun and Daoyuan Wu and Yue Xue and Han Liu and Wei Ma and Lyuye Zhang and Yang Liu and Yingjiu Li},
      year={2025},
      eprint={2401.16185},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2401.16185}, 
}

@misc{poppler,
  title = {Poppler - A PDF rendering library},
  author = {{Poppler Developers}},
  howpublished = {\url{https://poppler.freedesktop.org/}},
  note = {Accessed: 2025-01-25}
}

@misc{curl,
  title = {curl - A command-line tool and library for transferring data with URLs},
  author = {{curl Developers}},
  howpublished = {\url{https://curl.se/}},
  note = {Accessed: 2025-01-25}
}

@misc{svf,
  title = {SVF - Source Code Analysis with Static Value-Flow},
  author = {{SVF Developers}},
  howpublished = {\url{http://svf-tools.github.io/SVF/}},
  note = {Accessed: 2025-01-25}
}

@misc{pydot,
  title = {Pydot},
  author = {{Pydot Developers}},
  howpublished = {\url{https://pypi.org/project/pydot/}},
  note = {Accessed: 2025-01-25}
}
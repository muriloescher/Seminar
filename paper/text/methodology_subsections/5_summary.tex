This section presented the methodology used to investigate the augmentation of static analysis with large language models for enhanced vulnerability detection. The process was structured into several key steps:
\begin{enumerate}
    \item Dataset Selection: The BugOSS dataset was used as the basis for this study, providing a collection of real-world projects with known vulnerabilities. The dataset enabled reproducible experiments by including bug-inducing and bug-fixing commits alongside build scripts for each project.
    \item Static Analysis: Two widely used static analysis tools, CodeQL and Infer, were employed to detect vulnerabilities. While CodeQL struggled with C/C++ projects, Infer successfully identified a ground truth vulnerability in the PcapPlusPlus project. This project was selected as the focus of the subsequent analysis.
    \item Contextual Analysis: Using SVF, a call graph of the PcapPlusPlus project was generated to extract contextual information about flagged functions. Callers and callees of these functions were identified and consolidated for deeper reasoning about the flagged vulnerabilities.
    \item LLM Augmentation: The outputs from the static analysis tools and contextual analysis were combined into structured prompts for GPT-4o. By leveraging the LLM's reasoning capabilities, this step aimed to reduce false positives and enhance the interpretability of the analysis results.
\end{enumerate}

By integrating static analysis tools, contextual information, and a large language model, this methodology proposes an approach to addressing key limitations of traditional static analysis. The next section evaluates the effectiveness of this approach in terms of its ability to detect vulnerabilities and reduce false positives.
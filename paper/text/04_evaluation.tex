\section{Evaluation}
\label{sec:eval}
Each of the following topics presents the results of the subsections discussed in Section~\ref{sec:approach}.

\subsubsection{Project Builds}
As mentioned in Subsection~\ref{sec:approach:sub:dataset}, only 19 out of the 21 projects contained in BugOSS could be successfully built. These projects were then used with the static analyzers. The fact that not all projects could be built points to a flaw in BugOSS regarding reproducibility.

\subsubsection{Static Analysis Results}
To evaluate the detection capabilities of the static analysis tools, both CodeQL and Infer were run on the 19 successfully built projects. The number of flagged vulnerabilities for each project is summarized in Table~\ref{sast_results}. It is important to denote that Infer could not be executed on Harfbuzz due to its unsupported build system.

\begin{table}[ht]
\centering
\caption{Number of vulnerabilities flagged by CodeQL and Infer for each project.}
\label{sast_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Project Name} & \textbf{Flagged by CodeQL} & \textbf{Flagged by Infer} \\
\hline
Arrow & 0 & 71 \\
Aspell & 0 & 63 \\
Curl & 5 & 33 \\
Exiv2 & 3 & 55 \\
File & 0 & 107 \\
Gdal & 152 & 876 \\
Grok & - & - \\
Harfbuzz & 0 & - \\
Leptonica & 1 & 200 \\
Libarchive & 2 & 200 \\
Libhtp & 1 & 15 \\
Libxml2 & - & - \\
Ndpi & 5 & 75 \\
Openh264 & 11 & 191 \\
OpenSSL & 55 & 200 \\
PcapPlusPlus & 1 & 51 \\
Poppler & 15 & 200 \\
Readstat & 3 & 66 \\
Usrsctp & 2 & 0 \\
Yara & 1 & 24 \\
Zstd & 2 & 20 \\
\hline
Total & \textbf{259} & \textbf{2447} \\
\hline
\end{tabular}
\end{table}

As shown in the table, CodeQL flagged significantly fewer vulnerabilities compared to Infer. While Infer specializes in detecting memory-related vulnerabilities, CodeQL's default query set focuses on issues like SQL injection and cross-site scripting, which are uncommon in this dataset. CodeQL's analysis could possibly be improved with customized queries adapted to the nature of the projects in question.

The high amounts of flagged vulnerabilities by Infer is a great example as to why SAST tools present a challenge for a very effective and seamless continuous use. Sifting through so many errors, searching for true positives, proves to be very strenuous and time-consuming for developers. The approach presented in this paper hopes to reduce the number of errors which would then need to be manually checked.

\subsubsection{Gathering Contextual Information}
As previously mentioned 9 of the vulnerabilities flagged by Infer were found in the call graph and used for analysis in this step. Table~\ref{context} provides an overview of how many callers and callees each of the 9 functions had.

\begin{table}[ht]
\centering
\caption{Number of relevant callers and callees found per function analyzed.}
\label{context}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Function} & \textbf{\#Callers} & \textbf{\#Callees} \\
\hline
light\_create\_default\_file\_info & 1 & 0 \\
light\_create\_file\_info & 1 & 0 \\
light\_pcapng\_open\_read & 1 & 0 \\
light\_pcapng\_open\_write & 2 & 0 \\
pcpp::IDnsResource::decodeName & 1 & 4 \\
pcpp::IDnsResource::encodeName & 2 & 4 \\
pcpp::IPFilter::convertToIPAddressWithLen & 1 & 13 \\
pcpp::IPv4Layer::parseNextLayer & 8 & 37 \\
pcpp::IPv6Layer::parseExtensions & 3 & 11 \\
\hline
\end{tabular}
\end{table}

It is important to note that function calls pertaining to the C/C++ standard libraries, the LLVM compiler or any other compilation step, like an address sanitizer, that did not correspond to a function in the PcapPlusPlus repository, were not considered in this analysis, since they went out of the scope of the project.

As explained in Subsections~\ref{sec:approach:sub:context} and~\ref{sec:approach:sub:llm}, the source code for all these functions was gathered in separate files, whose paths were then given in the prompt.

\subsubsection{LLM Analysis}

Table~\ref{fps} shows, for each function, whether GPT-4o flagged the issue as a false positive or not, and the type of vulnerability originally marked by Infer.
\begin{table}[ht]
\centering
\caption{Qualitative Analysis from the LLM.}
\label{fps}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Function} & \textbf{False Positive?} & \textbf{Issue Type} \\
\hline
light\_create\_default\_file\_info & No & NULL\_DEREFERENCE \\
light\_create\_file\_info & No & NULL\_DEREFERENCE \\
light\_pcapng\_open\_read & No & NULL\_DEREFERENCE \\
light\_pcapng\_open\_write & No & NULL\_DEREFERENCE \\
pcpp::IDnsResource::decodeName & Yes & DEAD\_STORE \\
pcpp::IDnsResource::encodeName & Yes & DEAD\_STORE \\
pcpp::IPFilter::convertToIPAddressWithLen & No & DEAD\_STORE \\
pcpp::IPv4Layer::parseNextLayer & No & DEAD\_STORE \\
pcpp::IPv6Layer::parseExtensions & No & NULL\_DEREFERENCE \\
\hline
\end{tabular}
\end{table}

For all \textbf{NULL\_DEREFERENCE} type errors, the LLM provided a similar explanation as to why it is a true positive. For 3 of the 5 cases, a missing Null Check after a memory allocation is missing, and twice there was no check if the return of a function returning a pointer was null or not. All these warning can also be easily verified manually.

Regarding the \textbf{DEAD\_STORE} issues, there are some interesting observations, after human inspection of the issues is done:
\begin{itemize}
    \item For decodeName, GPT-4o outputs, in its conclusion: "The flagged issue appears to be a false positive in terms of security, as decodedNameLength is not used in a way that introduces a vulnerability. However, the presence of the variable as a dead store is indicative of inefficient or unclear code, and removing or repurposing it could improve maintainability.". So, despite detecting an unused value and agreeing with the static analysis result, which is incorrect after manually analyzing the code, the model correctly phrases it as it not actually being a true positive. 
    \item In regards to encodeName, the LLM recognizes the argument flagged by Infer as a pointer which is modified, not needing to be assigned to another variable or returned for it to be considered "used".
    \item Contrary to the previous point, GPT-4o does not recognize the flagged variables in convertToIPAddressWithLen as ones modifying a memory address, which is then later used.
    \item The LLM marks the issues in parseNextLayer as true positives. However, the flagged variables are clearly used, after manual inspection, for choosing which types of layers to instantiate.
\end{itemize}

The observations above highlight the fact that there is still room for improvement regarding the model's reasoning capabilities. The outputs were correct for the most simple type of issue, the null dereference, but not for the ones which demanded greater comprehension of the code.
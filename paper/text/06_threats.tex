\section{Threats to Validity}
\label{sec:threats}

In any empirical study, certain factors may impact the reliability and generalizability of the results. This section outlines the main limitations of the methodology.

\subsubsection{Limitations in Contextual Analysis}
The contextual analysis relied on SVF and LLVM bytecode to extract caller and callee relationships for the vulnerable functions. However, not all functions flagged by Infer were present in the call graph generated by SVF. This may indicate limitations in how SVF processes the bytecode or gaps in the way the call graph was constructed. Additionally, function names in the \texttt{.dot} file did not always match their actual representations in the source code, requiring manual mapping to align them correctly. This step introduced the possibility of errors that could impact the extracted context.

\subsubsection{Variability in LLM Responses}
LLMs such as GPT-4o are inherently non-deterministic, meaning that the same input may yield different responses across multiple runs. This poses a challenge in ensuring experimental reproducibility, as vulnerability classifications may vary if the experiment is repeated. A possible way to mitigate this issue would be to implement a voting-based approach or use multiple LLM instances to aggregate results, thereby reducing the impact of individual response variability.

\subsubsection{Manual Context Gathering}
The process of integrating static analysis results with contextual function relationships required some manual steps. Specifically, functions had to be manually filtered, formatted into structured JSON files, and cross-checked against the SVF-generated call graph. This introduces the risk of human error, which could lead to incorrect or missing contextual data. Automating these validation steps in future work would help improve reliability.
